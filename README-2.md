# Проект реализация собственной LLM.

**Цель написать пайплайны для обучения LLM: pretrain и SFT**

**Этапы проекта**

## Часть 1. Pretrain (предобучение). 

Цель: Научить модель генерировать осмысленный и стилистически корректный текст на основе корпуса русской классической литературы (JoannaBy/RussianNovels).

1.1. Скачать данные из репозитория https://github.com/JoannaBy/RussianNovels/tree/master/corpus и упаковать их в один датасет.

1.2. Провести препроцессинг данных.

1.3. Создать и обучить собственный токенизатор на полученных данных. Размер словаря выбрать небольшим.

1.4. Токенизация и формирование датасета.

1.5. Инициализация модели.

1.6. Обучение с валидацией на промптах. 

1.7. Сгенерировать ответы на запросы test_prompts.

## Часть 2. SFT (Supervised Fine-Tuning). 

Цель: Дообучить готовую предобученную модель на инструктивных данных, чтобы она могла корректно отвечать на вопросы на русском языке в формате «ассистент».

2.1. Загрузка базовой модели и токенизатора.

2.2. Подготовка инструктивного датасета, скачивание данных: d0rj/alpaca-cleaned-ru для обучения базовой модели.

2.3. Настройка SFT-обучения и обучение.

2.4. Оценка качества генерации. 